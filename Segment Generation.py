import xml.etree.ElementTree as ET
import pandas as pd
import numpy as np
import sys
import re

"""
This file converts its files to csv files, similar to the one generated by the LENA ADEX.
The segment duration can be specified by the user in seconds.

This is different from the code I used for the CDS paper, as the Praat code files were already generated before this conversion was attempted.
In the CDS paper, the 60-second segments were matched to the intervals in the TextGrid files, which were generated by Praat.
Because the intervals are not always 60 seconds, the segmentation needed extra consideration to account for extra outliers.

In this code, it is assumed that the manual coding has not yet been done.
The raw .ITS files will be segmented first neetly into x-second segments, on which the manual coding will be performed later.
"""

# File name for the raw ITS file and the output CSV file.
# Multiple ITS files can be specified in the list.
# The output CSV file will merge all the segments from the ITS files.
# Segments from different ITS files will be concatenated in the order they are listed, differentiated by the 'filename' column.
# Filename can be specified as command line arguments, otherwise the default files will be used.
FILENAME_LENA  = ['1.its'] if len(sys.argv) < 4 else sys.argv[3:]
FILENAME_CSV = 'Segmented output.csv'

# can fed as command line arguments, otherwise the default values will be used.
# example: python Segment_Generation.py OFFSET_SECONDS WINDOW_SECONDS FILENAME_LENA
OFFSET_SECONDS  = 0  if len(sys.argv) == 1 else sys.argv[1] # Offset in seconds to start the segments
WINDOW_SECONDS = 60  if len(sys.argv) <= 2 else sys.argv[2] # Duration of each output segment in seconds


# Names of the attributes inside the wrapper and their segment elements.
WRAPPER_COLS = ['childInitiation', 'childResponse', 'femaleAdultInitiation', 'femaleAdultResponse',
        'maleAdultInitiation', 'maleAdultResponse', 'turnTaking']
        
SEGMENT_COLS_BASIC = ['average_dB', 'peak_dB', 'childUttCnt', 
                      'femaleAdultUttCnt', 'femaleAdultWordCnt', 
                      'maleAdultUttCnt', 'maleAdultWordCnt']

SEGMENT_COLS_PARSING = ['childCryVfxLen', 'childUttLen', 
                'femaleAdultNonSpeechLen', 'femaleAdultUttLen', 
                'maleAdultNonSpeechLen',  'maleAdultUttLen']

ATTRIBUTE_COLS = ['Conversation', 'Pause', 'CHF', 'CHN', 'CXF', 'CXN', 
                  'FAF', 'FAN', 'MAF', 'MAN', 'NOF', 'NON', 'OLF', 'OLN', 'SIL', 'TVF', 'TVN']

# Get the normalization columns - these are the columns that will undergo numerical conversion (and summation).
NORMALIZED_COLS = list(set(WRAPPER_COLS + SEGMENT_COLS_BASIC + SEGMENT_COLS_PARSING + ATTRIBUTE_COLS) - {'average_dB', 'peak_dB'})

def retreive_segments_from_its(filename):
    # The LENA raw output follows the ISO 8601 format for time durations.
    # However, it is not strictly enforced, therefore existing parser such as isodate.parse_duration() would fail time to time.
    # The most frequent issue is that the duration is not in the correct format, such as "P123.45S" instead of "PT123.45S".
    # For the purpose of parsing the time in this code, we will use a simple custom function to parse the time by eleminating characters from the attibutes.
    def parse_time(t):
        # None can be passed, so we need to check if t is not None.
        if t:
            # Removes all non-numeric characters except '.' and converts to float.
            num_str = re.sub(r'[^\d.]', '', t)
            return float(num_str) if num_str else 0.0
        return 0.0
    root = ET.parse(filename).getroot() # Load ITS XML file from the root
    raw_data = []  # Reset the list to hold the raw data.

    # There can be multiple Recording elements in the ITS file.
    for rec in root.findall("./ProcessingUnit/Recording"):
        # Find all Conversation and Pause segment wrappers in the Recording elements
        for seg_wrapper in rec:
            # Calculate the wrapper duration, which is needed to normalize the segment attributes later.
            wrapper_duration = \
                    parse_time(seg_wrapper.attrib.get("endTime")) - \
                    parse_time(seg_wrapper.attrib.get("startTime"))
            
            # Iterate through each segment in the wrapper
            for seg in seg_wrapper:
                # Extract start and end times (in seconds)
                start_sec = parse_time(seg.attrib.get("startTime"))
                end_sec   = parse_time(seg.attrib.get("endTime"))
                duration = end_sec - start_sec

                # Create a dictionary to hold segment information.
                # This will be used to store the segment information in a structured way.
                # First, store specified information about the segment.
                segdict = {
                    "filename": filename, # Add the filename to the segment dictionary for later identification

                    "startTime": start_sec,
                    "endTime": end_sec,

                    "duration": duration,  # Duration of the segment in seconds
                    seg.attrib.get("spkr"): duration, # Use the speaker attribute as the key
                    seg_wrapper.tag: duration # record the duration of the segment wrapper type - either Conversation or Pause.
                }

                for col in SEGMENT_COLS_BASIC:
                    # The raw data does not always have the segment attributes, so we need to check if they exist.
                    segdict[col] = float(seg.attrib.get(col) or 0.0)  # Default to 0 if the attribute is not present
                
                # Second, record the segment attributes that require parsing.
                for col in SEGMENT_COLS_PARSING:
                    # Default to 0.0 if the attribute is not present
                    segdict[col] = parse_time(seg.attrib.get(col))
                
                # Lastly, record the segment attributes that are in the wrapper.
                # This requires additional consideration due to the attributes represent the whole wrapper, not just the segment.
                # Therefore, all information in the wrapper is normalized to the ratio of duration.
                for col in WRAPPER_COLS:
                    # Default to 0 if the attribute is not present
                    segdict[col] = parse_time(seg.attrib.get(col)) * (duration / wrapper_duration)

                
                raw_data.append(segdict)

    # Convert the raw data to a DataFrame
    df = pd.DataFrame(raw_data)

    # Find Child information that is static throughout the its file
    chld = root.find(".//ChildInfo")
    df["DoB"] = chld.attrib.get('dob')
    df["Gender"] = chld.attrib.get('gender') 
    df["Age_Month"] = parse_time(chld.attrib.get('chronologicalAge'))

    # Change DoB from obj to date
    df['DoB'] = pd.to_datetime(df['DoB'])

    # fill NaN values with 0
    df.fillna(0, inplace=True)

    return df

def fixed_length_segments(df, window, offset=0):
    # average_dB: weighted mean by duration
    def weighted_db(df):
        linear = 10 ** (pd.to_numeric(df['average_dB'], errors='coerce') / 10)
        weighted_linear = linear * df['duration']
        total_duration = df['duration'].sum()
        if total_duration > 0:
            overall_linear = weighted_linear.sum() / total_duration
            return 10 * np.log10(overall_linear)
        else:
            return np.nan
            
    # Find the starting and end times of the whole timeline
    timeline_start = df['startTime'].min() + offset
    timeline_end = df['endTime'].max()
    
    segments = []

    # Starting from the timeline start, create fixed-length segments
    window_start = timeline_start
    while window_start < timeline_end:
        window_end = window_start + window # End time of the current window

        # if window_end exceeds the timeline end, break the loop
        if window_end > timeline_end:
            print(f"Window end {window_end} exceeds timeline end {timeline_end}. Breaking the loop.")
            break

        # Get segments that partially overlap with the current window.
        df_partial = \
            df.copy()[   (df['startTime'] < window_end) & (df['endTime'] > window_start) & 
               ((df['startTime'] < window_start) | (df['endTime'] > window_end))]

        # Get segments that totally overlap with the current window (segment is within window).
        df_total = df.copy()[(df['startTime'] >= window_start) & (df['endTime'] <= window_end)]

        # There are two types of segment overlap:partial and total.
        # For partial overlaps, we need to split the overlapping segment to capture relative values of overlapping portions.
        # Once the partially overlapping segments are split, we can merge them with the total overlapping segments.
        # In case where a segment covers the whole window, there will be no 'total' df.

        # First, calculate the ratio of the overlapping segments
        df_partial.loc[:, 'overlap_ratio'] = \
            (np.minimum(df_partial['endTime'], window_end) - \
            np.maximum(df_partial['startTime'], window_start)) / df_partial['duration']

        # Now, normalize any numerical attributes in the df_partial DataFrame, except for the 'average_dB', 'peak_dB' columns.
        # They need special handling as the signal strength normalization is not linearly done.
        df_partial[NORMALIZED_COLS] = df_partial[NORMALIZED_COLS].mul(df_partial['overlap_ratio'], axis=0)

        # Now, reset some of the columns to better reflect the split segment.
        df_partial['duration'] = df_partial['overlap_ratio'] * df_partial['duration']
        df_partial["startTime"] = np.maximum(df_partial['startTime'], window_start)
        df_partial["endTime"]   = np.minimum(df_partial['endTime'], window_end)

        # Now concat the partial segments with the total segments.
        df_overlap = pd.concat([df_partial, df_total], ignore_index=True)

        # overlap ratio is no longer needed, as it is used to normalize the segment attributes.
        df_overlap.drop(columns=['overlap_ratio'], inplace=True, errors='ignore')

        # Calculate the average dB for segments in df_overlap.
        df_overlap['average_dB'] = weighted_db(df_overlap)

        # Introduce a new column to count the number of segments in each window.
        df_overlap['Seg_Count'] = 1  # Initialize with 1 for each segment

        # Aggregate the df_overlap to get the summarized segment data.
        # Note that 'filename', 'startTime', 'endTime', 'duration', 'DoB', 'Gender', 'Age_Month', 'average_dB', 'peak_dB' are not 'summed'.
        agg_dict = {
            "filename": lambda x: x.iloc[0],
            "startTime": lambda x: min(x),
            "endTime": lambda x: max(x),
            "duration": "sum",
            "Seg_Count": "sum",  # This will count the number of rows per group
            'DoB': lambda x: x.iloc[0], 
            'Gender': lambda x: x.iloc[0],
            'Age_Month': lambda x: x.iloc[0],
            "average_dB": lambda x: x.iloc[0],  # Average dB is the first value in the segment - it is already calculated
            "peak_dB": "max" # Peak dB is the maximum value in the segment
        }

        # add all remaining columns in NORMALIZED_COLS with 'sum'
        for col in NORMALIZED_COLS:
            agg_dict[col] = "sum"

        # Now, aggregate the df_overlap DataFrame using the agg_dict.
        # This will summarize the segments in the current window.
        segments.append(df_overlap.agg(agg_dict))

        window_start += window  # Move to the next window

    return segments


if __name__=="__main__":
    results = []
    for filename in FILENAME_LENA:
        print(f"Processing file: {filename} with window size {WINDOW_SECONDS} seconds and offset {OFFSET_SECONDS} seconds.")
        
        # Fist, retrieve segments from the ITS file in a dataframe format.
        # Relevant attributes are parsed and stored, recording duration of various speakers and segment types. 
        df_segments = retreive_segments_from_its(filename)

        # Now, feed the dataframe into the pre-determined time window segments.
        segments = fixed_length_segments(df_segments, WINDOW_SECONDS, OFFSET_SECONDS)
        print(f"Processing completed: {filename} has {len(segments)} fixed-length segments.")
        results.extend(segments)

    # Concatenate all segments into a single DataFrame
    segments_df = pd.DataFrame(results)

    # Save the segments to a CSV file
    segments_df.to_csv(FILENAME_CSV, index=False)
    print(f"Segments saved to {FILENAME_CSV}")

